{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ape (gpt-4o-mini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViHpAYFmKqPX"
      },
      "source": [
        "### Load dataset (GSM8K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5XQ222RG5rp",
        "outputId": "059f72be-1776-4701-a8d4-6efb5762206f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/toebee/development/Weavel/Ape-Starter-Template/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 500 items from trainset.jsonl\n",
            "Loaded 1319 items from testset.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/toebee/development/Weavel/Ape-Starter-Template/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"schema\" in \"JsonSchema\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n",
            "/Users/toebee/development/Weavel/Ape-Starter-Template/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:404: UserWarning: <built-in function allocate_lock> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import dotenv\n",
        "import json\n",
        "from dspy.datasets.gsm8k import parse_integer_answer\n",
        "from ape.types import DatasetItem\n",
        "\n",
        "# load environment variables\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Check if jsonl files exist\n",
        "# Load from jsonl files\n",
        "with open('trainset.jsonl', 'r') as f:\n",
        "    trainset = [DatasetItem(**json.loads(line)) for line in f]\n",
        "with open('testset.jsonl', 'r') as f:\n",
        "    testset = [DatasetItem(**json.loads(line)) for line in f]\n",
        "    \n",
        "print(f\"Loaded {len(trainset)} items from trainset.jsonl\")\n",
        "print(f\"Loaded {len(testset)} items from testset.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCFNEjSKKshY"
      },
      "source": [
        "### Setup evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WyOeJmRlHEih"
      },
      "outputs": [],
      "source": [
        "from ape import BaseMetric\n",
        "\n",
        "# Set up the metric\n",
        "class GSM8KMetric(BaseMetric):\n",
        "    def compute(self, inputs: dict, gold: dict, pred: dict, trace=None):\n",
        "        if not isinstance(pred, dict):\n",
        "            return False\n",
        "        if \"answer\" not in pred:\n",
        "            return False\n",
        "        return int(parse_integer_answer(str(gold[\"answer\"]))) == int(\n",
        "            parse_integer_answer(str(pred))\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9MAYB6K40y"
      },
      "source": [
        "### Setup MIPRO and load base prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ypMk5bSZHEcS"
      },
      "outputs": [],
      "source": [
        "from ape import MIPRO\n",
        "\n",
        "mipro = MIPRO(\n",
        "    prompt_model=\"gpt-4o\", # model that will generate instruction\n",
        "    task_model=\"gpt-4o-mini\", # model that will run the prompt\n",
        "    metric=GSM8KMetric(), # metric\n",
        "    verbose=True,\n",
        "    num_candidates=10, # number of candidate instructions that will be generated\n",
        "    minibatch_size=50, # number of examples to use in each minibatch\n",
        "    update_prompt_after_full_eval=False # set False to update prompt if evaluation on minibatch has highest score\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F6PKX8oyHGYv"
      },
      "outputs": [],
      "source": [
        "from ape import Prompt\n",
        "\n",
        "gsm8k_base_prompt = Prompt.load_file(\"gsm8k-base.prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmoRzfeKwbF"
      },
      "source": [
        "### Start Optimizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d1S2KaPTH-Yy",
        "outputId": "4d182057-e042-46cb-d2b6-af465b052430"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────────────────────── Cost Warning ──────────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">WARNING: Projected Language Model (LM) Calls</span>                                                                    │\n",
              "│                                                                                                                 │\n",
              "│ Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as     │\n",
              "│ follows:                                                                                                        │\n",
              "│                                                                                                                 │\n",
              "│ <span style=\"color: #008080; text-decoration-color: #008080\">- Prompt Model: 10 data summarizer calls + 10 lm calls in program = 21 prompt model calls</span>                       │\n",
              "│ <span style=\"color: #008080; text-decoration-color: #008080\">- Task Model: 50 examples in minibatch * 20 batches = 1000 task model calls</span>                                     │\n",
              "│                                                                                                                 │\n",
              "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Estimated Cost Calculation:</span>                                                                                     │\n",
              "│ Total Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input     │\n",
              "│ Token + Avg Output Token Length per Call * Task Model Price per Output Token)                                   │\n",
              "│             + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input │\n",
              "│ Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).                                │\n",
              "│                                                                                                                 │\n",
              "│ For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task │\n",
              "│ and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may         │\n",
              "│ consider:                                                                                                       │\n",
              "│ - Reducing the number of trials (`max_steps`), the size of the trainset, or the number of LM calls in your      │\n",
              "│ program.                                                                                                        │\n",
              "│ - Using a cheaper task model to optimize the prompt.                                                            │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "╭───────────────────────────────────────────────── Cost Warning ──────────────────────────────────────────────────╮\n",
              "│ \u001b[1;33mWARNING: Projected Language Model (LM) Calls\u001b[0m                                                                    │\n",
              "│                                                                                                                 │\n",
              "│ Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as     │\n",
              "│ follows:                                                                                                        │\n",
              "│                                                                                                                 │\n",
              "│ \u001b[36m- Prompt Model: 10 data summarizer calls + 10 lm calls in program = 21 prompt model calls\u001b[0m                       │\n",
              "│ \u001b[36m- Task Model: 50 examples in minibatch * 20 batches = 1000 task model calls\u001b[0m                                     │\n",
              "│                                                                                                                 │\n",
              "│ \u001b[1;33mEstimated Cost Calculation:\u001b[0m                                                                                     │\n",
              "│ Total Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input     │\n",
              "│ Token + Avg Output Token Length per Call * Task Model Price per Output Token)                                   │\n",
              "│             + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input │\n",
              "│ Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).                                │\n",
              "│                                                                                                                 │\n",
              "│ For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task │\n",
              "│ and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may         │\n",
              "│ consider:                                                                                                       │\n",
              "│ - Reducing the number of trials (`max_steps`), the size of the trainset, or the number of LM calls in your      │\n",
              "│ program.                                                                                                        │\n",
              "│ - Using a cheaper task model to optimize the prompt.                                                            │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m2024-08-29T07:52:46.221021Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError parsing outputs: Expecting ',' delimiter: line 6 column 626 (char 742)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mape.optimizer.utils\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mlogging.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m\n",
            "\u001b[2m2024-08-29T07:52:46.221616Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1m{\n",
            "  \"total_cans_collected\": 12,\n",
            "  \"cans_grandparents_house\": 36,\n",
            "  \"cans_from_neighbor\": 46,\n",
            "  \"cans_from_dad\": 250,\n",
            "  \"total_cans\": 12         \t  \t  \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u001b[0m [\u001b[0m\u001b[1m\u001b[34mape.optimizer.utils\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mlogging.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from ape.types import ResponseFormat\n",
        "from ape.types.response_format import JsonSchema\n",
        "\n",
        "optimized_prompt = await mipro.optimize(\n",
        "    student=gsm8k_base_prompt,\n",
        "    task_description=\"Solve math problems, come up with short factoid answers.\",\n",
        "    trainset=trainset,\n",
        "    testset=testset,\n",
        "    log_dir=\"./.gsm8k_logs\",  # all logs will be saved here\n",
        "    eval_kwargs={\n",
        "        \"max_errors\": 3,\n",
        "    },\n",
        "    max_bootstrapped_demos=5,  # maximum number of fewshot examples to use\n",
        "    max_labeled_demos=5,  # maximum number of labeled examples to use\n",
        "    max_steps=20,  # maximum number of optimization steps\n",
        "    goal_score=1.0,  # goal score to achieve, stop optimization if achieved\n",
        "    response_format=ResponseFormat(type=\"json_object\"),\n",
        "    requires_permission_to_run=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIvTRp4oQbeZ"
      },
      "source": [
        "### Evaluate Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yvaKrlNLQbEN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.934040940106141\n"
          ]
        }
      ],
      "source": [
        "from ape.evaluate import Evaluate\n",
        "import asyncio\n",
        "\n",
        "evaluate = Evaluate(testset, metric=GSM8KMetric())\n",
        "\n",
        "async def run_evaluation():\n",
        "    try:\n",
        "        score = await asyncio.wait_for(evaluate(optimized_prompt), timeout=300)  # 5 minutes timeout\n",
        "        print(score)\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Evaluation timed out after 5 minutes\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during evaluation: {str(e)}\")\n",
        "\n",
        "await run_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model='gpt-4o-mini' messages=[{'role': 'system', 'content': 'You are an expert math tutor proficient in solving word problems. Your task is to answer the given questions by breaking them down into detailed, step-by-step reasoning.\\n\\nYou must provide your output as a JSON object containing:\\n1. gold_reasoning: a step-by-step explanation of how to solve the problem.\\n2. answer: the final answer to the problem.\\n\\nEnsure that your reasoning is detailed, breaking down each part of the problem clearly and logically.'}, {'role': 'user', 'content': '{_FEWSHOT_}\\n\\nNow solve the following problem step by step:\\n\\nQuestion:\\n{question}'}] metadata={'inputs': {'question': 'The question to be answered.'}, 'outputs': {'answer': 'The answer to the question.'}, 'response_format': ResponseFormat(type='json_object', json_schema=None), 'fewshot': [DatasetItem(inputs={'question': 'Megan pays $16 for a shirt that costs $22 before sales. What is the amount of the discount?'}, outputs={'gold_reasoning': 'Let x be the amount of the discount. We have, 22 - x = $16 We change the writing of the equation: 22 - x + x = 16 + x So, 22 = 16 + x We then Remove 16 from both sides: 22 - 16 = 16 + x - 16 So, 22 - 16 = x So, the amount of the discount is x = $<<6=6>>6.', 'answer': '6'}, metadata={'split': 'train'}), DatasetItem(inputs={'question': 'Cathy has $12 left in her wallet. Her dad sent her $25 for her weekly consumption while her mom sent her twice the amount her dad sent her. How much money does Cathy have now?'}, outputs={'gold_reasoning': 'Cathy has $12 + $25 = $<<12+25=37>>37 after his father sent him money. Her mom sent her $25 x 2 = $<<25*2=50>>50. Therefore, Cathy has $50 + $37 = $<<50+37=87>>87.', 'answer': '87'}, metadata={'split': 'train'}), DatasetItem(inputs={'question': 'Mitzi brought $75 to the amusement park. She spent $30 on a ticket, $13 on food, and $23 on a T-shirt. How much money does she have left?'}, outputs={'gold_reasoning': 'Mitzi spent a total of $30 + $13 + $23 = $<<30+13+23=66>>66. So she still has $75 - $66 = $<<75-66=9>>9 left.', 'answer': '9'}, metadata={'split': 'train'}), DatasetItem(inputs={'question': \"Zhang is twice as old as Li.  Li is 12 years old.  Zhang's brother Jung is 2 years older than Zhang.  How old is Jung?\"}, outputs={'gold_reasoning': 'Zhang is 2 * 12 years old = <<2*12=24>>24 years old. Jung is 2 years + 24 years = <<2+24=26>>26 years old.', 'answer': '26'}, metadata={'split': 'train'}), DatasetItem(inputs={'question': \"Marla has to spend 20 minutes driving one way to her son's school, then 70 minutes attending parent-teacher night, then the same amount of time driving home. How many minutes does Marla spend on this errand in total?\"}, outputs={'gold_reasoning': \"First multiply Marla's one-way driving time by 2 to find her total driving time: 20 minutes * 2 = <<20*2=40>>40 minutes Then add Marla's driving time to the time spent attending the conference to find the total time: 70 minutes + 40 minutes = <<70+40=110>>110 minutes\", 'answer': '110'}, metadata={'split': 'train'})], 'name': 'gsm8k-base', 'trial_logs': {0: {'instruction': 1, 'fewshot': 2, 'prompt_path': './.gsm8k_logs/evaluated_prompts/prompt_0.prompt', 'num_eval_calls': 25, 'full_eval': False, 'score': 1.0, 'pruned': False, 'total_eval_calls_so_far': 25}}, 'score': 1.0, 'total_eval_calls': 25} temperature=0.0\n"
          ]
        }
      ],
      "source": [
        "print(optimized_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with CoT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4o-mini with CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ape.evaluate import Evaluate\n",
        "import asyncio\n",
        "\n",
        "cot_prompt_4o_mini = Prompt.load_file(\"gsm8k-cot-4o-mini.prompt\")\n",
        "\n",
        "evaluate = Evaluate(testset, metric=GSM8KMetric())\n",
        "\n",
        "async def run_evaluation():\n",
        "    try:\n",
        "        score = await asyncio.wait_for(evaluate(cot_prompt_4o_mini), timeout=300)  # 5 minutes timeout\n",
        "        print(score)\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Evaluation timed out after 5 minutes\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during evaluation: {str(e)}\")\n",
        "\n",
        "await run_evaluation()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
